"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
Object.defineProperty(exports, "MultiModalResponseSynthesizer", {
    enumerable: true,
    get: function() {
        return MultiModalResponseSynthesizer;
    }
});
const _schema = require("@llamaindex/core/schema");
const _utils = require("@llamaindex/core/utils");
const _Settings = require("../Settings.js");
const _Mixin = require("../prompts/Mixin.js");
const _Prompt = require("./../Prompt.js");
const _utils1 = require("./utils.js");
class MultiModalResponseSynthesizer extends _Mixin.PromptMixin {
    serviceContext;
    metadataMode;
    textQATemplate;
    constructor({ serviceContext, textQATemplate, metadataMode } = {}){
        super();
        this.serviceContext = serviceContext;
        this.metadataMode = metadataMode ?? _schema.MetadataMode.NONE;
        this.textQATemplate = textQATemplate ?? _Prompt.defaultTextQaPrompt;
    }
    _getPrompts() {
        return {
            textQATemplate: this.textQATemplate
        };
    }
    _updatePrompts(promptsDict) {
        if (promptsDict.textQATemplate) {
            this.textQATemplate = promptsDict.textQATemplate;
        }
    }
    async synthesize(query, stream) {
        const { nodesWithScore } = query;
        const nodes = nodesWithScore.map(({ node })=>node);
        const prompt = await (0, _utils1.createMessageContent)(this.textQATemplate, nodes, // fixme: wtf type is this?
        // { query },
        {}, this.metadataMode);
        const llm = (0, _Settings.llmFromSettingsOrContext)(this.serviceContext);
        if (stream) {
            const response = await llm.complete({
                prompt,
                stream
            });
            return (0, _utils.streamConverter)(response, ({ text })=>_schema.EngineResponse.fromResponse(text, true, nodesWithScore));
        }
        const response = await llm.complete({
            prompt
        });
        return _schema.EngineResponse.fromResponse(response.text, false, nodesWithScore);
    }
}

import { HfInference } from "@huggingface/inference";
import { BaseEmbedding } from "@llamaindex/core/embeddings";
export declare enum HuggingFaceEmbeddingModelType {
    XENOVA_ALL_MINILM_L6_V2 = "Xenova/all-MiniLM-L6-v2",
    XENOVA_ALL_MPNET_BASE_V2 = "Xenova/all-mpnet-base-v2"
}
/**
 * Uses feature extraction from '@xenova/transformers' to generate embeddings.
 * Per default the model [XENOVA_ALL_MINILM_L6_V2](https://huggingface.co/Xenova/all-MiniLM-L6-v2) is used.
 *
 * Can be changed by setting the `modelType` parameter in the constructor, e.g.:
 * ```
 * new HuggingFaceEmbedding({
 *     modelType: HuggingFaceEmbeddingModelType.XENOVA_ALL_MPNET_BASE_V2,
 * });
 * ```
 *
 * @extends BaseEmbedding
 */
export declare class HuggingFaceEmbedding extends BaseEmbedding {
    modelType: string;
    quantized: boolean;
    private extractor;
    constructor(init?: Partial<HuggingFaceEmbedding>);
    getExtractor(): Promise<any>;
    getTextEmbedding(text: string): Promise<number[]>;
}
type HfInferenceOptions = ConstructorParameters<typeof HfInference>[1];
export type HFConfig = HfInferenceOptions & {
    model: string;
    accessToken: string;
    endpoint?: string;
};
/**
 * Uses feature extraction from Hugging Face's Inference API to generate embeddings.
 *
 * Set the `model` and `accessToken` parameter in the constructor, e.g.:
 * ```
 * new HuggingFaceInferenceAPIEmbedding({
 *     model: HuggingFaceEmbeddingModelType.XENOVA_ALL_MPNET_BASE_V2,
 *     accessToken: "<your-access-token>"
 * });
 * ```
 *
 * @extends BaseEmbedding
 */
export declare class HuggingFaceInferenceAPIEmbedding extends BaseEmbedding {
    model: string;
    hf: HfInference;
    constructor(init: HFConfig);
    getTextEmbedding(text: string): Promise<number[]>;
    getTextEmbeddings: (texts: string[]) => Promise<Array<number[]>>;
}
export {};

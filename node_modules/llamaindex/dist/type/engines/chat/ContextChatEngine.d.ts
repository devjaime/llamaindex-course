import type { ChatMessage, LLM, MessageType } from "@llamaindex/core/llms";
import { EngineResponse } from "@llamaindex/core/schema";
import type { ChatHistory } from "../../ChatHistory.js";
import type { ContextSystemPrompt } from "../../Prompt.js";
import type { BaseRetriever } from "../../Retriever.js";
import type { BaseNodePostprocessor } from "../../postprocessors/index.js";
import { PromptMixin } from "../../prompts/Mixin.js";
import type { ChatEngine, ChatEngineParamsNonStreaming, ChatEngineParamsStreaming, ContextGenerator } from "./types.js";
/**
 * ContextChatEngine uses the Index to get the appropriate context for each query.
 * The context is stored in the system prompt, and the chat history is chunk: ChatResponseChunk, nodes?: NodeWithScore<import("/Users/marcus/code/llamaindex/LlamaIndexTS/packages/core/src/Node").Metadata>[], nodes?: NodeWithScore<import("/Users/marcus/code/llamaindex/LlamaIndexTS/packages/core/src/Node").Metadata>[]lowing the appropriate context to be surfaced for each query.
 */
export declare class ContextChatEngine extends PromptMixin implements ChatEngine {
    chatModel: LLM;
    chatHistory: ChatHistory;
    contextGenerator: ContextGenerator;
    systemPrompt?: string;
    constructor(init: {
        retriever: BaseRetriever;
        chatModel?: LLM;
        chatHistory?: ChatMessage[];
        contextSystemPrompt?: ContextSystemPrompt;
        nodePostprocessors?: BaseNodePostprocessor[];
        systemPrompt?: string;
        contextRole?: MessageType;
    });
    protected _getPromptModules(): Record<string, ContextGenerator>;
    chat(params: ChatEngineParamsStreaming): Promise<AsyncIterable<EngineResponse>>;
    chat(params: ChatEngineParamsNonStreaming): Promise<EngineResponse>;
    reset(): void;
    private prepareRequestMessages;
    private prependSystemPrompt;
}

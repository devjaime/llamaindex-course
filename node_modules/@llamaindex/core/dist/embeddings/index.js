import { TransformComponent, MetadataMode } from '../schema/index.js';
import { extractSingleText } from '../utils/index.js';
import { tokenizers } from '@llamaindex/env';

function truncateMaxTokens(tokenizer, value, maxTokens) {
    // the maximum number of tokens per one character is 2 (e.g. 爨)
    if (value.length * 2 < maxTokens) return value;
    const t = tokenizers.tokenizer(tokenizer);
    let tokens = t.encode(value);
    if (tokens.length > maxTokens) {
        // truncate tokens
        tokens = tokens.slice(0, maxTokens);
        value = t.decode(tokens);
        // if we truncate at an UTF-8 boundary (some characters have more than one token), tiktoken returns a � character - remove it
        return value.replace("�", "");
    }
    return value;
}

const DEFAULT_SIMILARITY_TOP_K = 2;
var SimilarityType;
(function(SimilarityType) {
    SimilarityType["DEFAULT"] = "cosine";
    SimilarityType["DOT_PRODUCT"] = "dot_product";
    SimilarityType["EUCLIDEAN"] = "euclidean";
})(SimilarityType || (SimilarityType = {}));
/**
 * The similarity between two embeddings.
 * @param embedding1
 * @param embedding2
 * @param mode
 * @returns similarity score with higher numbers meaning the two embeddings are more similar
 */ function similarity(embedding1, embedding2, mode = "cosine") {
    if (embedding1.length !== embedding2.length) {
        throw new Error("Embedding length mismatch");
    }
    // NOTE I've taken enough Kahan to know that we should probably leave the
    // numeric programming to numeric programmers. The naive approach here
    // will probably cause some avoidable loss of floating point precision
    // ml-distance is worth watching although they currently also use the naive
    // formulas
    function norm(x) {
        let result = 0;
        for(let i = 0; i < x.length; i++){
            result += x[i] * x[i];
        }
        return Math.sqrt(result);
    }
    switch(mode){
        case "euclidean":
            {
                const difference = embedding1.map((x, i)=>x - embedding2[i]);
                return -norm(difference);
            }
        case "dot_product":
            {
                let result = 0;
                for(let i = 0; i < embedding1.length; i++){
                    result += embedding1[i] * embedding2[i];
                }
                return result;
            }
        case "cosine":
            {
                return similarity(embedding1, embedding2, "dot_product") / (norm(embedding1) * norm(embedding2));
            }
        default:
            throw new Error("Not implemented yet");
    }
}

const DEFAULT_EMBED_BATCH_SIZE = 10;
class BaseEmbedding extends TransformComponent {
    constructor(){
        super(async (nodes, options)=>{
            const texts = nodes.map((node)=>node.getContent(MetadataMode.EMBED));
            const embeddings = await this.getTextEmbeddingsBatch(texts, options);
            for(let i = 0; i < nodes.length; i++){
                nodes[i].embedding = embeddings[i];
            }
            return nodes;
        });
        this.embedBatchSize = DEFAULT_EMBED_BATCH_SIZE;
        /**
   * Optionally override this method to retrieve multiple embeddings in a single request
   * @param texts
   */ this.getTextEmbeddings = async (texts)=>{
            const embeddings = [];
            for (const text of texts){
                const embedding = await this.getTextEmbedding(text);
                embeddings.push(embedding);
            }
            return embeddings;
        };
    }
    similarity(embedding1, embedding2, mode = SimilarityType.DEFAULT) {
        return similarity(embedding1, embedding2, mode);
    }
    async getQueryEmbedding(query) {
        const text = extractSingleText(query);
        if (text) {
            return await this.getTextEmbedding(text);
        }
        return null;
    }
    /**
   * Get embeddings for a batch of texts
   * @param texts
   * @param options
   */ async getTextEmbeddingsBatch(texts, options) {
        return await batchEmbeddings(texts, this.getTextEmbeddings, this.embedBatchSize, options);
    }
    truncateMaxTokens(input) {
        return input.map((s)=>{
            // truncate to max tokens
            if (!(this.embedInfo?.tokenizer && this.embedInfo?.maxTokens)) return s;
            return truncateMaxTokens(this.embedInfo.tokenizer, s, this.embedInfo.maxTokens);
        });
    }
}
async function batchEmbeddings(values, embedFunc, chunkSize, options) {
    const resultEmbeddings = [];
    const queue = values;
    const curBatch = [];
    for(let i = 0; i < queue.length; i++){
        curBatch.push(queue[i]);
        if (i == queue.length - 1 || curBatch.length == chunkSize) {
            const embeddings = await embedFunc(curBatch);
            resultEmbeddings.push(...embeddings);
            if (options?.logProgress) {
                console.log(`getting embedding progress: ${i} / ${queue.length}`);
            }
            curBatch.length = 0;
        }
    }
    return resultEmbeddings;
}

export { BaseEmbedding, DEFAULT_SIMILARITY_TOP_K, SimilarityType, batchEmbeddings, similarity, truncateMaxTokens };

import { Settings } from "@llamaindex/core/global";
import { baseToolWithCallSchema } from "@llamaindex/core/schema";
import { ReadableStream } from "@llamaindex/env";
import { z } from "zod";
import { isAsyncIterable, prettifyError, stringifyJSONToMessageContent } from "../internal/utils.js";
// #TODO stepTools and stepToolsStreaming should be moved to a better abstraction
export async function stepToolsStreaming({ response, tools, step, enqueueOutput }) {
    const responseChunkStream = new ReadableStream({
        async start (controller) {
            for await (const chunk of response){
                controller.enqueue(chunk);
            }
            controller.close();
        }
    });
    const [pipStream, finalStream] = responseChunkStream.tee();
    const reader = pipStream.getReader();
    const { value } = await reader.read();
    reader.releaseLock();
    if (value === undefined) {
        throw new Error("first chunk value is undefined, this should not happen");
    }
    // check if first chunk has tool calls, if so, this is a function call
    // otherwise, it's a regular message
    const hasToolCall = !!(value.options && "toolCall" in value.options);
    enqueueOutput({
        taskStep: step,
        output: finalStream,
        isLast: !hasToolCall
    });
    if (hasToolCall) {
        // you need to consume the response to get the full toolCalls
        const toolCalls = new Map();
        for await (const chunk of pipStream){
            if (chunk.options && "toolCall" in chunk.options) {
                const toolCall = chunk.options.toolCall;
                toolCall.forEach((toolCall)=>{
                    toolCalls.set(toolCall.id, toolCall);
                });
            }
        }
        // If there are toolCalls, but they didn't get read into the stream, used for Gemini
        if (!toolCalls.size && value.options && "toolCall" in value.options) {
            value.options.toolCall.forEach((toolCall)=>{
                toolCalls.set(toolCall.id, toolCall);
            });
        }
        step.context.store.messages = [
            ...step.context.store.messages,
            {
                role: "assistant",
                content: "",
                options: {
                    toolCall: [
                        ...toolCalls.values()
                    ]
                }
            }
        ];
        for (const toolCall of toolCalls.values()){
            const targetTool = tools.find((tool)=>tool.metadata.name === toolCall.name);
            const toolOutput = await callTool(targetTool, toolCall, step.context.logger);
            step.context.store.messages = [
                ...step.context.store.messages,
                {
                    role: "user",
                    content: stringifyJSONToMessageContent(toolOutput.output),
                    options: {
                        toolResult: {
                            result: toolOutput.output,
                            isError: toolOutput.isError,
                            id: toolCall.id
                        }
                    }
                }
            ];
            step.context.store.toolOutputs.push(toolOutput);
        }
    }
}
export async function stepTools({ response, tools, step, enqueueOutput }) {
    step.context.store.messages = [
        ...step.context.store.messages,
        response.message
    ];
    const options = response.message.options ?? {};
    enqueueOutput({
        taskStep: step,
        output: response,
        isLast: !("toolCall" in options)
    });
    if ("toolCall" in options) {
        const { toolCall } = options;
        for (const call of toolCall){
            const targetTool = tools.find((tool)=>tool.metadata.name === call.name);
            const toolOutput = await callTool(targetTool, call, step.context.logger);
            step.context.store.toolOutputs.push(toolOutput);
            step.context.store.messages = [
                ...step.context.store.messages,
                {
                    content: stringifyJSONToMessageContent(toolOutput.output),
                    role: "user",
                    options: {
                        toolResult: {
                            result: toolOutput.output,
                            isError: toolOutput.isError,
                            id: call.id
                        }
                    }
                }
            ];
        }
    }
}
export async function callTool(tool, toolCall, logger) {
    let input;
    if (typeof toolCall.input === "string") {
        try {
            input = JSON.parse(toolCall.input);
        } catch (e) {
            const output = `Tool ${toolCall.name} can't be called. Input is not a valid JSON object.`;
            logger.error(`${output} Try increasing the maxTokens parameter of your LLM. Invalid Input: ${toolCall.input}`);
            return {
                tool,
                input: {},
                output,
                isError: true
            };
        }
    } else {
        input = toolCall.input;
    }
    if (!tool) {
        logger.error(`Tool ${toolCall.name} does not exist.`);
        const output = `Tool ${toolCall.name} does not exist.`;
        return {
            tool,
            input,
            output,
            isError: true
        };
    }
    const call = tool.call;
    let output;
    if (!call) {
        logger.error(`Tool ${tool.metadata.name} (remote:${toolCall.name}) does not have a implementation.`);
        output = `Tool ${tool.metadata.name} (remote:${toolCall.name}) does not have a implementation.`;
        return {
            tool,
            input,
            output,
            isError: true
        };
    }
    try {
        Settings.callbackManager.dispatchEvent("llm-tool-call", {
            toolCall: {
                ...toolCall,
                input
            }
        });
        output = await call.call(tool, input);
        logger.log(`Tool ${tool.metadata.name} (remote:${toolCall.name}) succeeded.`);
        logger.log(`Output: ${JSON.stringify(output)}`);
        const toolOutput = {
            tool,
            input,
            output,
            isError: false
        };
        Settings.callbackManager.dispatchEvent("llm-tool-result", {
            toolCall: {
                ...toolCall,
                input
            },
            toolResult: {
                ...toolOutput
            }
        });
        return toolOutput;
    } catch (e) {
        output = prettifyError(e);
        logger.error(`Tool ${tool.metadata.name} (remote:${toolCall.name}) failed: ${output}`);
    }
    return {
        tool,
        input,
        output,
        isError: true
    };
}
export async function consumeAsyncIterable(input, previousContent = "") {
    if (isAsyncIterable(input)) {
        const result = {
            content: previousContent,
            // only assistant will give streaming response
            role: "assistant",
            options: {}
        };
        for await (const chunk of input){
            result.content += chunk.delta;
            if (chunk.options) {
                result.options = {
                    ...result.options,
                    ...chunk.options
                };
            }
        }
        return result;
    } else {
        return input;
    }
}
export function createReadableStream(asyncIterable) {
    return new ReadableStream({
        async start (controller) {
            for await (const chunk of asyncIterable){
                controller.enqueue(chunk);
            }
            controller.close();
        }
    });
}
export function validateAgentParams(params) {
    if ("tools" in params) {
        z.array(baseToolWithCallSchema).parse(params.tools);
    } else {
    // todo: check `params.toolRetriever` when migrate to @llamaindex/core
    }
}

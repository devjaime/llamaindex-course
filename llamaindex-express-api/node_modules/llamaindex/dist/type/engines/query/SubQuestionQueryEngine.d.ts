import { EngineResponse } from "@llamaindex/core/schema";
import type { ServiceContext } from "../../ServiceContext.js";
import { PromptMixin } from "../../prompts/Mixin.js";
import type { BaseSynthesizer } from "../../synthesizers/index.js";
import type { BaseTool, ToolMetadata } from "@llamaindex/core/llms";
import type { BaseQueryEngine, QueryType } from "@llamaindex/core/query-engine";
import type { BaseQuestionGenerator } from "./types.js";
/**
 * SubQuestionQueryEngine decomposes a question into subquestions and then
 */
export declare class SubQuestionQueryEngine extends PromptMixin implements BaseQueryEngine {
    responseSynthesizer: BaseSynthesizer;
    questionGen: BaseQuestionGenerator;
    queryEngines: BaseTool[];
    metadatas: ToolMetadata[];
    constructor(init: {
        questionGen: BaseQuestionGenerator;
        responseSynthesizer: BaseSynthesizer;
        queryEngineTools: BaseTool[];
    });
    protected _getPromptModules(): Record<string, any>;
    static fromDefaults(init: {
        queryEngineTools: BaseTool[];
        questionGen?: BaseQuestionGenerator;
        responseSynthesizer?: BaseSynthesizer;
        serviceContext?: ServiceContext;
    }): SubQuestionQueryEngine;
    query(query: QueryType, stream: true): Promise<AsyncIterable<EngineResponse>>;
    query(query: QueryType, stream?: false): Promise<EngineResponse>;
    private querySubQ;
}

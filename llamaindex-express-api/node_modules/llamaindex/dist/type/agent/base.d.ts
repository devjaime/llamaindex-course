import type { BaseToolWithCall, ChatMessage, LLM, MessageContent } from "@llamaindex/core/llms";
import { EngineResponse } from "@llamaindex/core/schema";
import { ReadableStream } from "@llamaindex/env";
import { type ChatEngine, type ChatEngineParamsNonStreaming, type ChatEngineParamsStreaming } from "../engines/chat/index.js";
import { ObjectRetriever } from "../objects/index.js";
import type { AgentTaskContext, TaskHandler, TaskStep, TaskStepOutput } from "./types.js";
export declare const MAX_TOOL_CALLS = 10;
export declare function createTaskOutputStream<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never>(handler: TaskHandler<Model, Store, AdditionalMessageOptions>, context: AgentTaskContext<Model, Store, AdditionalMessageOptions>): ReadableStream<TaskStepOutput<Model, Store, AdditionalMessageOptions>>;
export type AgentRunnerParams<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    llm: AI;
    chatHistory: ChatMessage<AdditionalMessageOptions>[];
    systemPrompt: MessageContent | null;
    runner: AgentWorker<AI, Store, AdditionalMessageOptions>;
    tools: BaseToolWithCall[] | ((query: MessageContent) => Promise<BaseToolWithCall[]>);
    verbose: boolean;
};
export type AgentParamsBase<AI extends LLM, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    llm?: AI;
    chatHistory?: ChatMessage<AdditionalMessageOptions>[];
    systemPrompt?: MessageContent;
    verbose?: boolean;
    tools: BaseToolWithCall[];
} | {
    llm?: AI;
    chatHistory?: ChatMessage<AdditionalMessageOptions>[];
    systemPrompt?: MessageContent;
    verbose?: boolean;
    toolRetriever: ObjectRetriever<BaseToolWithCall>;
};
/**
 * Worker will schedule tasks and handle the task execution
 */
export declare abstract class AgentWorker<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> {
    #private;
    abstract taskHandler: TaskHandler<AI, Store, AdditionalMessageOptions>;
    createTask(query: MessageContent, context: AgentTaskContext<AI, Store, AdditionalMessageOptions>): ReadableStream<TaskStepOutput<AI, Store, AdditionalMessageOptions>>;
    [Symbol.toStringTag]: string;
}
/**
 * Runner will manage the task execution and provide a high-level API for the user
 */
export declare abstract class AgentRunner<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> implements ChatEngine {
    #private;
    abstract createStore(): Store;
    static defaultCreateStore(): object;
    static defaultTaskHandler: TaskHandler<LLM>;
    protected constructor(params: AgentRunnerParams<AI, Store, AdditionalMessageOptions>);
    get llm(): AI;
    get chatHistory(): ChatMessage<AdditionalMessageOptions>[];
    get verbose(): boolean;
    reset(): void;
    getTools(query: MessageContent): Promise<BaseToolWithCall[]> | BaseToolWithCall[];
    static shouldContinue<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never>(task: Readonly<TaskStep<AI, Store, AdditionalMessageOptions>>): boolean;
    createTask(message: MessageContent, stream?: boolean, verbose?: boolean | undefined, chatHistory?: ChatMessage<AdditionalMessageOptions>[]): ReadableStream<TaskStepOutput<AI, Store, AdditionalMessageOptions>>;
    chat(params: ChatEngineParamsNonStreaming): Promise<EngineResponse>;
    chat(params: ChatEngineParamsStreaming): Promise<ReadableStream<EngineResponse>>;
}

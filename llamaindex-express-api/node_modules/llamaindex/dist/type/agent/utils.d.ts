import type { BaseTool, ChatMessage, ChatResponse, ChatResponseChunk, LLM, PartialToolCall, TextChatMessage, ToolCall, ToolCallLLMMessageOptions, ToolOutput } from "@llamaindex/core/llms";
import { ReadableStream } from "@llamaindex/env";
import type { Logger } from "../internal/logger.js";
import type { AgentParamsBase } from "./base.js";
import type { TaskHandler } from "./types.js";
type StepToolsResponseParams<Model extends LLM> = {
    response: ChatResponse<ToolCallLLMMessageOptions>;
    tools: BaseTool[];
    step: Parameters<TaskHandler<Model, {}, ToolCallLLMMessageOptions>>[0];
    enqueueOutput: Parameters<TaskHandler<Model, {}, ToolCallLLMMessageOptions>>[1];
};
type StepToolsStreamingResponseParams<Model extends LLM> = Omit<StepToolsResponseParams<Model>, "response"> & {
    response: AsyncIterable<ChatResponseChunk<ToolCallLLMMessageOptions>>;
};
export declare function stepToolsStreaming<Model extends LLM>({ response, tools, step, enqueueOutput, }: StepToolsStreamingResponseParams<Model>): Promise<void>;
export declare function stepTools<Model extends LLM>({ response, tools, step, enqueueOutput, }: StepToolsResponseParams<Model>): Promise<void>;
export declare function callTool(tool: BaseTool | undefined, toolCall: ToolCall | PartialToolCall, logger: Logger): Promise<ToolOutput>;
export declare function consumeAsyncIterable<Options extends object>(input: ChatMessage<Options>, previousContent?: string): Promise<ChatMessage<Options>>;
export declare function consumeAsyncIterable<Options extends object>(input: AsyncIterable<ChatResponseChunk<Options>>, previousContent?: string): Promise<TextChatMessage<Options>>;
export declare function createReadableStream<T>(asyncIterable: AsyncIterable<T>): ReadableStream<T>;
export declare function validateAgentParams<AI extends LLM>(params: AgentParamsBase<AI>): void;
export {};
